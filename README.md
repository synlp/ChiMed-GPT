[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README.md) | [**ğŸŒEnglish**](./README-EN.md)

# ChiMed-GPT

ChiMed-GPT æ˜¯ä¸€æ¬¾åŸºäº [Ziya-v2](https://arxiv.org/abs/2311.03301) çš„ä¸­æ–‡åŒ»ç–—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚åœ¨Ziya-v2çš„åŸºç¡€ä¸Šï¼Œç»¼åˆè¿›è¡Œäº†é¢„è®­ç»ƒã€ç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰å’Œæ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚

æ›´å¤šå…³äºè¯¥æ¨¡å‹çš„ä¿¡æ¯å³å°†å‘å¸ƒã€‚

å¦‚æ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ–è€…å¯¹ ChiMed-GPT æœªæ¥çš„ç‰ˆæœ¬æœ‰ä»»ä½•å»ºè®®ï¼Œè¯·åœ¨ issue ä¸­ç•™è¨€ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨æˆ–æ‰©å±•æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·å¼•ç”¨ä»¥ä¸‹[è®ºæ–‡](https://arxiv.org/abs/2311.06025)
```
@article{USTC-ChiMed-GPT,
  title="{ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences}",
  author={Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, Yongdong Zhang},
  journal={arXiv preprint arXiv:2311.06025},
  year={2023},
}
```

## è®­ç»ƒè¿‡ç¨‹

ChiMed-GPT çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬äº†é¢„è®­ç»ƒã€ç›‘ç£å¾®è°ƒã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ã€‚å…·ä½“æµç¨‹å’Œä½¿ç”¨çš„æ•°æ®é›†å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](docs/figures/architecture.png)

## ç»“æœ

æˆ‘ä»¬åœ¨ä¿¡æ¯æå–ã€é—®ç­”ï¼ˆQAï¼‰å’Œå¤šè½®å¯¹è¯ä¸Šè¯„ä¼°äº† ChiMed-GPTã€‚

### ä¿¡æ¯æŠ½å–

æˆ‘ä»¬åœ¨å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ä»»åŠ¡ä¸Šï¼Œåœ¨ CCKS2019 å’Œ [ChiMST](https://github.com/synlp/ChiMST) ä¸Šçš„ç»“æœä¸º

| æ¨¡å‹             | CCKS-2019 | ChiMST |
|-----------------|-----------|--------|
| GPT-3.5-Turbo   | 31.42     | 32.15  |
| GPT-4           | 41.37     | 41.25  |
| Ziya-v1         | 25.31     | 22.26  |
| Ziya-v2         | 27.84     | 25.76  |
| Baichuan        | 24.14     | 21.20  |
| Taiyi           | 30.90     | 30.55  |
| MedicalGPT (Z)  | 29.59     | 28.12  |
| MedicalGPT (B)  | 23.80     | 26.16  |
| CHiMed-GPT      | **40.82** | **41.04** |

### QA

åœ¨ [C-Eval](https://cevalbenchmark.com/)ã€CMMLU å’Œ MedQA ä¸Šçš„ç»“æœä¸º

| æ¨¡å‹             | C-Eval | CMMLU | MedQA |
|----------------|--------|-------|-------|
| GPT-3.5-Turbo  | 56.58  | 49.91 | 44.50 |
| GPT-4          | 71.29  | 69.55 | 67.99 |
| Ziya-v1        | 36.59  | 29.07 | 12.50 |
| Ziya-v2        | 39.02  | 49.06 | 13.00 |
| Baichuan       | 41.46  | 45.28 | 13.00 |
| Taiyi          | 48.78  | 45.20 | 39.20 |
| MedicalGPT (Z) | 48.78  | 34.56 | 25.99 |
| MedicalGPT (B) | 39.02  | 43.82 | 18.50 |
| CHiMed-GPT     | **68.29** | **52.92** | **44.50** |

ä»¥åŠåœ¨ [ChiMed](https://github.com/synlp/ChiMST) ä¸Šçš„ç»“æœä¸º

| æ¨¡å‹         | BLEU-1  | BLEU-2  | ROUGE-1  | ROUGE-2  | ROUGE-L  |
|----------------|------|------|------|------|------|
| GPT-3.5-Turbo  | 39.15| 32.85| 26.61| 7.31 | 16.84|
| GPT-4          | 33.61| 28.27| 26.51| 7.13 | 16.63|
| Ziya-v1        | 6.18 | 5.77 | 18.59| 3.94 | 12.66|
| Ziya-v2        | 38.41| 31.90| 26.91| 7.90 | 18.67|
| Baichuan       | 5.81 | 5.25 | 16.91| 3.01 | 11.30|
| Taiyi          | 11.73| 9.96 | 21.76| 5.26 | 15.46|
| MedicalGPT (Z) | 39.02| 32.35| 26.76| 8.10 | 18.16|
| MedicalGPT (B) | 5.82 | 5.26 | 16.61| 2.94 | 11.11|
| CHiMed-GPT     | **44.58**| **37.22**| **27.11**| **8.89** | **19.86**|

### å¤šè½®å¯¹è¯

åœ¨ [MC](https://aclanthology.org/2020.coling-main.63/) ä¸Šçš„ç»“æœ

| æ¨¡å‹             | B-1   | B-2   | R-1   | R-2  | R-L  |
|-----------------|-------|-------|-------|------|------|
| GPT-3.5-Turbo   | 24.29 | 20.17 | 20.64 | 8.39 | 17.14|
| GPT-4           | 18.58 | 15.76 | 18.92 | 6.62 | 14.55|
| Ziya-v1         | 15.85 | 11.75 | 9.92  | 3.04 | 9.02 |
| Ziya-v2         | 14.21 | 10.99 | 12.20 | 4.45 | 10.61|
| Baichuan        | 3.44  | 1.61  | 3.87  | 0.34 | 3.49 |
| Taiyi           | 5.81  | 4.67  | 14.23 | 4.55 | 11.99|
| MedicalGPT (Z)  | 20.26 | 16.42 | 17.51 | 5.42 | 14.21|
| MedicalGPT (B)  | 3.94  | 2.19  | 4.34  | 0.13 | 3.50 |
| CHiMed-GPT      | **33.14** | **30.86** | **43.43** | **34.91**| **42.16**|

## ä¸‹è½½

1.0 ç‰ˆæœ¬å·²åœ¨ [Hugging Face](https://huggingface.co/SYNLP/ChiMed-GPT-1.0) å‘å¸ƒã€‚

## ä½¿ç”¨æ–¹æ³•

å®‰è£…æ ¹æ®å®˜ç½‘çš„æ•™ç¨‹å®‰è£… [PyTroch](https://pytorch.org/get-started/locally/) å’Œ [Transformers](https://huggingface.co/docs/transformers/installation) å¹¶ä½¿ç”¨ä¸‹é¢çš„ä»£ç è¿è¡Œ

```python
from transformers import AutoTokenizer
from transformers import LlamaForCausalLM
import torch

query="[human]:æ„Ÿå†’æ€ä¹ˆå¤„ç†ï¼Ÿ\n[bot]:"
model = LlamaForCausalLM.from_pretrained('SYNLP/ChiMed-GPT-1.0', torch_dtype=torch.float16, device_map="auto").eval()
tokenizer = AutoTokenizer.from_pretrained(ckpt)
input_ids = tokenizer(query, return_tensors="pt").input_ids.to('cuda:0')
generate_ids = model.generate(
            input_ids,
            max_new_tokens=512, 
            do_sample = True, 
            top_p = 0.9)
output = tokenizer.batch_decode(generate_ids)[0]
print(output)
```

æ³¨ï¼šè¯·ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ `transformers` (æˆ‘ä»¬ä½¿ç”¨çš„ç‰ˆæœ¬ä¸º4.35.2)

## å…è´£å£°æ˜

è¯·æ³¨æ„ï¼ŒChiMed-GPT æä¾›çš„æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬å»ºè®®å’Œæ¨èç­‰ï¼Œéƒ½ä¸ä»£è¡¨æˆ‘ä»¬çš„ç«‹åœºã€‚æˆ‘ä»¬ä¸å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åšå‡ºçš„å›ç­”æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚ç”¨æˆ·åº”æ˜ç™½ï¼ŒChiMed-GPTå¹¶ä¸æ˜¯ä¸“ä¸šçš„åŒ»ç”Ÿã€‚ç”¨æˆ·åº”ä¾æ®è‡ªå·±çš„åˆ¤æ–­ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä¿¡æ¯ã€‚è‹¥æœ‰å¿…è¦ï¼Œåº”åŠæ—¶å‰å¾€åŒ»é™¢å°±è¯Šå¹¶å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿçš„æ„è§ã€‚

